{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###1) job_csv-parquet"
      ],
      "metadata": {
        "id": "b_2gw0emVGM1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OI1RDxn9NyLS"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext, SparkConf\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "\n",
        "## @params: [JOB_NAME]\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
        "\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)\n",
        "\n",
        "# Lendo o arquivo CSV da camada RAW do bucket S3\n",
        "df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"sep\", \"|\") \\\n",
        "    .load(\"s3://projeto-compass-filmes/Raw/Local/CSV/Movies/2022/05/02/movies.csv\")\n",
        "\n",
        "# Filtrando as linhas com base na lista de IDs de filmes do IMDB\n",
        "lista_franquias = ['tt0120737', 'tt0167261', 'tt0167260', 'tt0903624', 'tt1170358', 'tt2310332', 'tt2310332', 'tt0133093', 'tt0234215', 'tt0242653', 'tt10838180', 'tt0088247', 'tt0103064', 'tt0181852', 'tt0438488', 'tt1340138', 'tt6450804', 'tt0088763', 'tt0096874', 'tt0099088', 'tt0076759', 'tt0080684', 'tt0086190', 'tt0120915', 'tt0121765', 'tt0121766', 'tt2488496', 'tt2527336', 'tt2527338', 'tt0079501', 'tt0082694', 'tt0089530', 'tt1392190', 'tt0078748', 'tt0090605', 'tt0103644', 'tt0118583', 'tt1446714', 'tt2316204']\n",
        "movies = df.filter(df['id'].isin(lista_franquias))\n",
        "\n",
        "# Removendo colunas\n",
        "colunas_para_remover = ['tituloOriginal', 'anoLancamento', 'tempoMinutos',\n",
        "                        'genero', 'notaMedia', 'numeroVotos',\n",
        "                        'generoArtista','titulosMaisConhecidos']\n",
        "movies_franquias = movies.drop(*colunas_para_remover)\n",
        "\n",
        "# Salvando o arquivo como movies.parquet no bucket S3\n",
        "movies_franquias.write.mode('overwrite').parquet(\"s3://projeto-compass-filmes/TRT/Movies/movies.parquet\")\n",
        "\n",
        "job.commit()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### job_json-parquet"
      ],
      "metadata": {
        "id": "YPoIAMSSVMU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "## @params: [JOB_NAME]\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
        "\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)\n",
        "\n",
        "# Insira seu c√≥digo aqui\n",
        "\n",
        "# Lendo o arquivo JSON da camada RAW do bucket S3\n",
        "df = spark.read.json(\"s3://projeto-compass-filmes/Raw/TMDB/JSON/2023/06/26/franquias.json\")\n",
        "\n",
        "# Adicionando a coluna \"dt\" com o valor da data desejada\n",
        "df = df.withColumn(\"dt\", lit(\"2023-06-26\"))\n",
        "\n",
        "# Escrevendo o arquivo em formato Parquet na camada TRUSTED do bucket S3, particionado por data\n",
        "df.write.partitionBy(\"dt\").parquet(\"s3://projeto-compass-filmes/TRT/franquias.parquet\")\n",
        "\n",
        "job.commit()"
      ],
      "metadata": {
        "id": "1vz4518ib3Tk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}